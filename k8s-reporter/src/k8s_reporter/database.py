"""
Database client for reading k8s-analyzer SQLite databases.

This module provides a high-level interface for querying the SQLite databases
generated by k8s-analyzer, with methods optimized for web UI consumption.
"""

import json
import sqlite3
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import pandas as pd

from k8s_reporter.models import (
    ClusterOverview,
    ResourceSummary,
    NamespaceAnalysis,
    NamespaceComponent,
    NamespaceRelationship,
    NamespaceComponentsView,
    SecurityAnalysis,
    ResourceEfficiency,
    PodResourceIssue,
    ComplianceReport,
    StorageVolume,
    StorageConsumption,
    NamespaceStorageAnalysis,
    ResourceTimeline,
    TemporalAnalysis,
)


class DatabaseClient:
    """Client for reading k8s-analyzer SQLite databases."""
    
    def __init__(self, db_path: str):
        """Initialize database client.
        
        Args:
            db_path: Path to the SQLite database file
        """
        self.db_path = Path(db_path)
        if not self.db_path.exists():
            raise FileNotFoundError(f"Database file not found: {db_path}")
    
    def get_connection(self) -> sqlite3.Connection:
        """Get database connection with row factory."""
        conn = sqlite3.connect(str(self.db_path))
        conn.row_factory = sqlite3.Row
        return conn
    
    def get_resource_summary(self) -> ResourceSummary:
        """Get overall resource summary."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Total resources and relationships
            cursor.execute("SELECT COUNT(*) FROM resources")
            total_resources = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM relationships")
            total_relationships = cursor.fetchone()[0]
            
            # Health distribution
            cursor.execute("""
                SELECT health_status, COUNT(*) as count
                FROM resources
                GROUP BY health_status
            """)
            health_distribution = {row['health_status']: row['count'] for row in cursor.fetchall()}
            
            # Resource types
            cursor.execute("""
                SELECT kind, COUNT(*) as count
                FROM resources
                GROUP BY kind
                ORDER BY count DESC
            """)
            resource_types = {row['kind']: row['count'] for row in cursor.fetchall()}
            
            # Namespaces count
            cursor.execute("""
                SELECT COUNT(DISTINCT namespace) as count
                FROM resources
                WHERE namespace IS NOT NULL
            """)
            namespaces_count = cursor.fetchone()[0]
            
            # Issues count
            cursor.execute("""
                SELECT COUNT(*) as count
                FROM resources
                WHERE issues IS NOT NULL AND issues != '[]'
            """)
            issues_count = cursor.fetchone()[0]
            
            return ResourceSummary(
                total_resources=total_resources,
                total_relationships=total_relationships,
                health_distribution=health_distribution,
                resource_types=resource_types,
                namespaces_count=namespaces_count,
                issues_count=issues_count
            )
    
    def get_cluster_overview(self) -> ClusterOverview:
        """Get high-level cluster overview."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Get analysis timestamp
            cursor.execute("""
                SELECT analysis_timestamp
                FROM analysis_summary
                ORDER BY created_at DESC
                LIMIT 1
            """)
            result = cursor.fetchone()
            analysis_timestamp = datetime.fromisoformat(result[0]) if result else datetime.now(timezone.utc)
            
            # Total resources and namespaces
            cursor.execute("SELECT COUNT(*) FROM resources")
            total_resources = cursor.fetchone()[0]
            
            cursor.execute("""
                SELECT COUNT(DISTINCT namespace)
                FROM resources
                WHERE namespace IS NOT NULL
            """)
            total_namespaces = cursor.fetchone()[0]
            
            # Health ratio
            cursor.execute("""
                SELECT 
                    SUM(CASE WHEN health_status = 'healthy' THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as ratio
                FROM resources
            """)
            health_ratio = cursor.fetchone()[0] or 0.0
            
            # Top namespaces
            cursor.execute("""
                SELECT namespace, COUNT(*) as count
                FROM resources
                WHERE namespace IS NOT NULL
                GROUP BY namespace
                ORDER BY count DESC
                LIMIT 10
            """)
            top_namespaces = [{"name": row['namespace'], "count": row['count']} for row in cursor.fetchall()]
            
            # Resource distribution
            cursor.execute("""
                SELECT kind, COUNT(*) as count
                FROM resources
                GROUP BY kind
                ORDER BY count DESC
            """)
            resource_distribution = {row['kind']: row['count'] for row in cursor.fetchall()}
            
            # Issues summary
            cursor.execute("""
                SELECT health_status, COUNT(*) as count
                FROM resources
                WHERE health_status != 'healthy'
                GROUP BY health_status
            """)
            issues_summary = {row['health_status']: row['count'] for row in cursor.fetchall()}
            
            return ClusterOverview(
                analysis_timestamp=analysis_timestamp,
                total_resources=total_resources,
                total_namespaces=total_namespaces,
                health_ratio=health_ratio,
                top_namespaces=top_namespaces,
                resource_distribution=resource_distribution,
                issues_summary=issues_summary
            )
    
    def get_namespace_analysis(self, namespace: str) -> Optional[NamespaceAnalysis]:
        """Get detailed analysis for a specific namespace."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Check if namespace exists
            cursor.execute("SELECT COUNT(*) FROM resources WHERE namespace = ?", (namespace,))
            if cursor.fetchone()[0] == 0:
                return None
            
            # Resource count
            cursor.execute("SELECT COUNT(*) FROM resources WHERE namespace = ?", (namespace,))
            resource_count = cursor.fetchone()[0]
            
            # Resource types
            cursor.execute("""
                SELECT kind, COUNT(*) as count
                FROM resources
                WHERE namespace = ?
                GROUP BY kind
                ORDER BY count DESC
            """, (namespace,))
            resource_types = {row['kind']: row['count'] for row in cursor.fetchall()}
            
            # Health distribution
            cursor.execute("""
                SELECT health_status, COUNT(*) as count
                FROM resources
                WHERE namespace = ?
                GROUP BY health_status
            """, (namespace,))
            health_distribution = {row['health_status']: row['count'] for row in cursor.fetchall()}
            
            # Issues count
            cursor.execute("""
                SELECT COUNT(*) as count
                FROM resources
                WHERE namespace = ? AND issues IS NOT NULL AND issues != '[]'
            """, (namespace,))
            issues_count = cursor.fetchone()[0]
            
            # Relationships count
            cursor.execute("""
                SELECT COUNT(*) as count
                FROM relationships
                WHERE source_namespace = ? OR target_namespace = ?
            """, (namespace, namespace))
            relationships_count = cursor.fetchone()[0]
            
            # Top resources
            cursor.execute("""
                SELECT name, kind, health_status
                FROM resources
                WHERE namespace = ?
                ORDER BY 
                    CASE health_status 
                        WHEN 'error' THEN 1 
                        WHEN 'warning' THEN 2 
                        ELSE 3 
                    END,
                    name
                LIMIT 10
            """, (namespace,))
            top_resources = [
                {"name": row['name'], "kind": row['kind'], "health": row['health_status']}
                for row in cursor.fetchall()
            ]
            
            return NamespaceAnalysis(
                name=namespace,
                resource_count=resource_count,
                resource_types=resource_types,
                health_distribution=health_distribution,
                issues_count=issues_count,
                relationships_count=relationships_count,
                top_resources=top_resources
            )
    
    def get_security_analysis(self) -> SecurityAnalysis:
        """Get security-focused analysis."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Service accounts
            cursor.execute("SELECT COUNT(*) FROM resources WHERE kind = 'ServiceAccount'")
            service_accounts_count = cursor.fetchone()[0]
            
            # Role bindings
            cursor.execute("SELECT COUNT(*) FROM resources WHERE kind = 'RoleBinding'")
            role_bindings_count = cursor.fetchone()[0]
            
            # ConfigMaps and Secrets (approximation)
            cursor.execute("SELECT COUNT(*) FROM resources WHERE kind = 'ConfigMap'")
            config_maps_count = cursor.fetchone()[0]
            
            cursor.execute("SELECT COUNT(*) FROM resources WHERE kind = 'Secret'")
            secrets_count = cursor.fetchone()[0]
            
            # For pods analysis, we need to parse the spec JSON
            cursor.execute("""
                SELECT spec
                FROM resources
                WHERE kind = 'Pod' AND spec IS NOT NULL
            """)
            
            privileged_pods = 0
            pods_without_security_context = 0
            root_containers = 0
            
            for row in cursor.fetchall():
                try:
                    spec = json.loads(row['spec'])
                    containers = spec.get('containers', [])
                    
                    has_security_context = False
                    for container in containers:
                        security_context = container.get('securityContext', {})
                        if security_context:
                            has_security_context = True
                            if security_context.get('privileged', False):
                                privileged_pods += 1
                            if security_context.get('runAsUser') == 0:
                                root_containers += 1
                    
                    if not has_security_context:
                        pods_without_security_context += 1
                        
                except (json.JSONDecodeError, KeyError):
                    continue
            
            return SecurityAnalysis(
                privileged_pods=privileged_pods,
                pods_without_security_context=pods_without_security_context,
                root_containers=root_containers,
                service_accounts_count=service_accounts_count,
                role_bindings_count=role_bindings_count,
                cluster_role_bindings=0,  # Not available in current schema
                secrets_count=secrets_count,
                config_maps_count=config_maps_count
            )
    
    def get_resources_dataframe(self, filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """Get resources as pandas DataFrame for analysis."""
        query = "SELECT * FROM resources WHERE 1=1"
        params = []
        
        if filters:
            if filters.get('namespace'):
                query += " AND namespace = ?"
                params.append(filters['namespace'])
            if filters.get('kind'):
                query += " AND kind = ?"
                params.append(filters['kind'])
            if filters.get('health_status'):
                query += " AND health_status = ?"
                params.append(filters['health_status'])
        
        with self.get_connection() as conn:
            return pd.read_sql_query(query, conn, params=params)
    
    def get_relationships_dataframe(self, filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:
        """Get relationships as pandas DataFrame for analysis."""
        query = "SELECT * FROM relationships WHERE 1=1"
        params = []
        
        if filters:
            if filters.get('source_namespace'):
                query += " AND source_namespace = ?"
                params.append(filters['source_namespace'])
            if filters.get('relationship_type'):
                query += " AND relationship_type = ?"
                params.append(filters['relationship_type'])
        
        with self.get_connection() as conn:
            return pd.read_sql_query(query, conn, params=params)
    
    def get_namespaces(self) -> List[str]:
        """Get list of all namespaces."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT namespace
                FROM resources
                WHERE namespace IS NOT NULL
                ORDER BY namespace
            """)
            return [row[0] for row in cursor.fetchall()]
    
    def get_resource_kinds(self) -> List[str]:
        """Get list of all resource kinds."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT DISTINCT kind
                FROM resources
                ORDER BY kind
            """)
            return [row[0] for row in cursor.fetchall()]
    
    def get_health_over_time(self) -> pd.DataFrame:
        """Get health status changes over time."""
        with self.get_connection() as conn:
            query = """
                SELECT 
                    DATE(timestamp) as date,
                    health_status,
                    COUNT(*) as count
                FROM resource_health_history
                GROUP BY DATE(timestamp), health_status
                ORDER BY date
            """
            return pd.read_sql_query(query, conn)
    
    def search_resources(self, search_term: str, limit: int = 50) -> List[Dict[str, Any]]:
        """Search resources by name or kind."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            cursor.execute("""
                SELECT name, namespace, kind, health_status, uid
                FROM resources
                WHERE name LIKE ? OR kind LIKE ?
                ORDER BY 
                    CASE health_status 
                        WHEN 'error' THEN 1 
                        WHEN 'warning' THEN 2 
                        ELSE 3 
                    END,
                    name
                LIMIT ?
            """, (f"%{search_term}%", f"%{search_term}%", limit))
            
            return [dict(row) for row in cursor.fetchall()]
    
    def get_namespace_components_view(self, namespace: str) -> Optional[NamespaceComponentsView]:
        """Get detailed components view for a specific namespace."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Check if namespace exists
            cursor.execute("SELECT COUNT(*) FROM resources WHERE namespace = ?", (namespace,))
            if cursor.fetchone()[0] == 0:
                return None
            
            # Get all components in the namespace
            cursor.execute("""
                SELECT name, kind, health_status, labels, issues, creation_timestamp, uid
                FROM resources
                WHERE namespace = ?
                ORDER BY kind, name
            """, (namespace,))
            
            components = []
            component_uids = {}  # Map name to uid for relationship lookup
            
            for row in cursor.fetchall():
                # Parse labels and issues from JSON
                labels = {}
                issues = []
                try:
                    if row['labels']:
                        labels = json.loads(row['labels'])
                    if row['issues']:
                        issues = json.loads(row['issues'])
                except json.JSONDecodeError:
                    pass
                
                component = NamespaceComponent(
                    name=row['name'],
                    kind=row['kind'],
                    health_status=row['health_status'],
                    labels=labels,
                    issues=issues,
                    created_at=row['creation_timestamp']
                )
                components.append(component)
                component_uids[row['name']] = row['uid']
            
            # Get relationships within the namespace
            cursor.execute("""
                SELECT 
                    r.source_uid,
                    r.target_resource,
                    r.relationship_type,
                    r.strength,
                    r.description,
                    source_res.name as source_name,
                    source_res.kind as source_kind,
                    target_res.name as target_name,
                    target_res.kind as target_kind
                FROM relationships r
                JOIN resources source_res ON r.source_uid = source_res.uid
                LEFT JOIN resources target_res ON r.target_resource = target_res.uid
                WHERE source_res.namespace = ?
                   OR target_res.namespace = ?
                ORDER BY r.relationship_type, source_res.name
            """, (namespace, namespace))
            
            relationships = []
            for row in cursor.fetchall():
                if row['target_name']:  # Only include if target exists
                    relationship = NamespaceRelationship(
                        source_name=row['source_name'],
                        source_kind=row['source_kind'],
                        target_name=row['target_name'],
                        target_kind=row['target_kind'],
                        relationship_type=row['relationship_type'],
                        strength=row['strength'] or 1.0,
                        description=row['description']
                    )
                    relationships.append(relationship)
            
            # Group components by type
            component_groups = {}
            for component in components:
                if component.kind not in component_groups:
                    component_groups[component.kind] = []
                component_groups[component.kind].append(component.name)
            
            # Find orphaned components (no relationships)
            connected_components = set()
            for rel in relationships:
                connected_components.add(rel.source_name)
                connected_components.add(rel.target_name)
            
            orphaned_components = [
                comp.name for comp in components 
                if comp.name not in connected_components
            ]
            
            # Find critical components (high number of relationships)
            component_relationship_count = {}
            for rel in relationships:
                component_relationship_count[rel.source_name] = component_relationship_count.get(rel.source_name, 0) + 1
                component_relationship_count[rel.target_name] = component_relationship_count.get(rel.target_name, 0) + 1
            
            # Components with 3+ relationships are considered critical
            critical_components = [
                name for name, count in component_relationship_count.items()
                if count >= 3
            ]
            
            # Build dependency chains (simplified - just direct chains)
            dependency_chains = self._build_dependency_chains(relationships)
            
            return NamespaceComponentsView(
                namespace=namespace,
                total_components=len(components),
                components=components,
                relationships=relationships,
                component_groups=component_groups,
                dependency_chains=dependency_chains,
                orphaned_components=orphaned_components,
                critical_components=critical_components
            )
    
    def _build_dependency_chains(self, relationships: List[NamespaceRelationship]) -> List[List[str]]:
        """Build dependency chains from relationships."""
        # Create adjacency list
        graph = {}
        for rel in relationships:
            if rel.source_name not in graph:
                graph[rel.source_name] = []
            graph[rel.source_name].append(rel.target_name)
        
        # Find chains (simplified approach - find paths of length 2+)
        chains = []
        visited = set()
        
        def dfs_chain(node, current_chain):
            if len(current_chain) > 1 and node in visited:
                return
            
            visited.add(node)
            current_chain.append(node)
            
            if node in graph:
                for neighbor in graph[node]:
                    dfs_chain(neighbor, current_chain.copy())
            
            if len(current_chain) > 2:
                chains.append(current_chain)
        
        # Start DFS from each node
        for node in graph:
            if node not in visited:
                dfs_chain(node, [])
        
        # Remove duplicates and keep only meaningful chains
        unique_chains = []
        for chain in chains:
            if len(chain) >= 2 and chain not in unique_chains:
                unique_chains.append(chain)
        
        return unique_chains[:10]  # Limit to 10 chains
    
    def get_storage_consumption(self) -> StorageConsumption:
        """Get global storage consumption analysis."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Get all storage-related resources
            cursor.execute("""
                SELECT name, namespace, kind, spec, status, labels, creation_timestamp
                FROM resources
                WHERE kind IN ('PersistentVolume', 'PersistentVolumeClaim')
                ORDER BY name
            """)
            
            volumes = []
            total_capacity_gb = 0.0
            used_capacity_gb = 0.0
            volumes_by_class = {}
            capacity_by_class = {}
            volumes_by_status = {}
            unbound_pvcs = 0
            orphaned_pvs = 0
            
            for row in cursor.fetchall():
                spec = {}
                status = {}
                labels = {}
                
                try:
                    if row['spec']:
                        spec = json.loads(row['spec'])
                    if row['status']:
                        status = json.loads(row['status'])
                    if row['labels']:
                        labels = json.loads(row['labels'])
                except json.JSONDecodeError:
                    pass
                
                # Extract capacity
                capacity_str = None
                capacity_gb = 0.0
                
                if row['kind'] == 'PersistentVolume':
                    capacity = spec.get('capacity', {}).get('storage', '')
                    capacity_str = capacity
                elif row['kind'] == 'PersistentVolumeClaim':
                    requests = spec.get('resources', {}).get('requests', {})
                    capacity = requests.get('storage', '')
                    capacity_str = capacity
                
                if capacity_str:
                    capacity_gb = self._parse_storage_size(capacity_str)
                    total_capacity_gb += capacity_gb
                
                # Extract storage class
                storage_class = spec.get('storageClassName', 'default')
                if storage_class not in volumes_by_class:
                    volumes_by_class[storage_class] = 0
                    capacity_by_class[storage_class] = 0.0
                volumes_by_class[storage_class] += 1
                capacity_by_class[storage_class] += capacity_gb
                
                # Extract access modes
                access_modes = spec.get('accessModes', [])
                
                # Determine status
                volume_status = 'unknown'
                if row['kind'] == 'PersistentVolume':
                    volume_status = status.get('phase', 'unknown').lower()
                    if volume_status == 'available':
                        orphaned_pvs += 1
                elif row['kind'] == 'PersistentVolumeClaim':
                    volume_status = status.get('phase', 'unknown').lower()
                    if volume_status == 'pending':
                        unbound_pvcs += 1
                
                if volume_status not in volumes_by_status:
                    volumes_by_status[volume_status] = 0
                volumes_by_status[volume_status] += 1
                
                # Create volume object
                volume = StorageVolume(
                    name=row['name'],
                    namespace=row['namespace'],
                    kind=row['kind'],
                    capacity=capacity_str,
                    storage_class=storage_class,
                    access_modes=access_modes,
                    status=volume_status,
                    created_at=datetime.fromisoformat(row['creation_timestamp']) if row['creation_timestamp'] else None,
                    labels=labels
                )
                volumes.append(volume)
            
            # Calculate utilization (simplified - assume used = total for bound volumes)
            bound_volumes = [v for v in volumes if v.status in ['bound', 'available']]
            used_capacity_gb = sum(
                self._parse_storage_size(v.capacity or '0Gi') 
                for v in bound_volumes if v.capacity
            )
            
            utilization_percentage = (used_capacity_gb / total_capacity_gb * 100) if total_capacity_gb > 0 else 0
            
            # Get top consumers (largest volumes)
            volumes.sort(key=lambda v: self._parse_storage_size(v.capacity or '0Gi'), reverse=True)
            top_consumers = volumes[:10]
            
            return StorageConsumption(
                total_volumes=len(volumes),
                total_capacity_gb=total_capacity_gb,
                used_capacity_gb=used_capacity_gb,
                utilization_percentage=utilization_percentage,
                volumes_by_class=volumes_by_class,
                capacity_by_class=capacity_by_class,
                volumes_by_status=volumes_by_status,
                unbound_pvcs=unbound_pvcs,
                orphaned_pvs=orphaned_pvs,
                top_consumers=top_consumers
            )
    
    def get_namespace_storage_analysis(self, namespace: str) -> Optional[NamespaceStorageAnalysis]:
        """Get storage analysis for a specific namespace."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Get storage resources in namespace
            cursor.execute("""
                SELECT name, kind, spec, status, labels, creation_timestamp
                FROM resources
                WHERE namespace = ? AND kind IN ('PersistentVolumeClaim')
                ORDER BY name
            """, (namespace,))
            
            volumes = []
            total_capacity_gb = 0.0
            storage_classes = {}
            access_patterns = {}
            volume_status = {}
            
            for row in cursor.fetchall():
                spec = {}
                status = {}
                labels = {}
                
                try:
                    if row['spec']:
                        spec = json.loads(row['spec'])
                    if row['status']:
                        status = json.loads(row['status'])
                    if row['labels']:
                        labels = json.loads(row['labels'])
                except json.JSONDecodeError:
                    pass
                
                # Extract details
                requests = spec.get('resources', {}).get('requests', {})
                capacity_str = requests.get('storage', '')
                capacity_gb = self._parse_storage_size(capacity_str) if capacity_str else 0.0
                total_capacity_gb += capacity_gb
                
                storage_class = spec.get('storageClassName', 'default')
                storage_classes[storage_class] = storage_classes.get(storage_class, 0) + 1
                
                access_modes = spec.get('accessModes', [])
                for mode in access_modes:
                    access_patterns[mode] = access_patterns.get(mode, 0) + 1
                
                pvc_status = status.get('phase', 'unknown').lower()
                volume_status[pvc_status] = volume_status.get(pvc_status, 0) + 1
                
                volume = StorageVolume(
                    name=row['name'],
                    namespace=namespace,
                    kind=row['kind'],
                    capacity=capacity_str,
                    storage_class=storage_class,
                    access_modes=access_modes,
                    status=pvc_status,
                    created_at=datetime.fromisoformat(row['creation_timestamp']) if row['creation_timestamp'] else None,
                    labels=labels
                )
                volumes.append(volume)
            
            if not volumes:
                return None
            
            # Get largest volumes
            volumes.sort(key=lambda v: self._parse_storage_size(v.capacity or '0Gi'), reverse=True)
            largest_volumes = volumes[:5]
            
            return NamespaceStorageAnalysis(
                namespace=namespace,
                total_volumes=len(volumes),
                total_capacity_gb=total_capacity_gb,
                storage_classes=storage_classes,
                access_patterns=access_patterns,
                volume_status=volume_status,
                volumes=volumes,
                largest_volumes=largest_volumes
            )
    
    def _parse_storage_size(self, size_str: str) -> float:
        """Parse Kubernetes storage size string to GB."""
        if not size_str:
            return 0.0
        
        # Remove whitespace and convert to lowercase
        size_str = size_str.strip().lower()
        
        # Extract number and unit
        import re
        match = re.match(r'([0-9.]+)([a-z]*)', size_str)
        if not match:
            return 0.0
        
        value = float(match.group(1))
        unit = match.group(2)
        
        # Convert to GB
        conversions = {
            'ki': value / (1024 * 1024),  # KiB to GB
            'mi': value / 1024,           # MiB to GB  
            'gi': value,                  # GiB to GB (approximately)
            'ti': value * 1024,           # TiB to GB
            'k': value / (1000 * 1000),   # KB to GB
            'm': value / 1000,            # MB to GB
            'g': value,                   # GB
            't': value * 1000,            # TB to GB
            '': value / (1024 * 1024 * 1024),  # Bytes to GB
        }
        
        return conversions.get(unit, value)
    
    def get_temporal_analysis(self, days_back: int = 30) -> TemporalAnalysis:
        """Get temporal analysis of resource lifecycle patterns."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Calculate date range
            from datetime import timedelta
            end_date = datetime.now(timezone.utc)
            start_date = end_date - timedelta(days=days_back)
            
            # Get all resources with timestamps
            cursor.execute("""
                SELECT name, kind, namespace, creation_timestamp, uid
                FROM resources
                WHERE creation_timestamp IS NOT NULL
                ORDER BY creation_timestamp
            """)
            
            all_resources = []
            creation_timeline = []
            newest_resources = []
            oldest_resources = []
            stale_resources = []
            
            # Process resources
            for row in cursor.fetchall():
                try:
                    created_at = datetime.fromisoformat(row['creation_timestamp'])
                    # Ensure both datetimes are timezone-aware for comparison
                    if created_at.tzinfo is None:
                        created_at = created_at.replace(tzinfo=timezone.utc)
                    age_days = (end_date - created_at).days
                    
                    # Determine lifecycle stage
                    if age_days <= 1:
                        lifecycle_stage = 'new'
                    elif age_days <= 7:
                        lifecycle_stage = 'recent'
                    elif age_days <= 30:
                        lifecycle_stage = 'active'
                    elif age_days <= 90:
                        lifecycle_stage = 'mature'
                    else:
                        lifecycle_stage = 'stale'
                    
                    timeline_resource = ResourceTimeline(
                        resource_name=row['name'],
                        resource_kind=row['kind'],
                        namespace=row['namespace'],
                        created_at=created_at,
                        age_days=age_days,
                        lifecycle_stage=lifecycle_stage
                    )
                    
                    all_resources.append(timeline_resource)
                    
                    # Build creation timeline
                    creation_timeline.append({
                        'date': created_at.strftime('%Y-%m-%d'),
                        'kind': row['kind'],
                        'namespace': row['namespace'],
                        'count': 1
                    })
                    
                    # Categorize resources
                    if age_days <= 7:
                        newest_resources.append(timeline_resource)
                    elif age_days >= 90:
                        oldest_resources.append(timeline_resource)
                    elif age_days >= 60:  # No updates for 60+ days
                        stale_resources.append(timeline_resource)
                        
                except ValueError:
                    continue
            
            # Aggregate creation timeline by day
            timeline_by_date = {}
            for entry in creation_timeline:
                date = entry['date']
                if date not in timeline_by_date:
                    timeline_by_date[date] = {'date': date, 'total': 0, 'by_kind': {}}
                timeline_by_date[date]['total'] += 1
                kind = entry['kind']
                if kind not in timeline_by_date[date]['by_kind']:
                    timeline_by_date[date]['by_kind'][kind] = 0
                timeline_by_date[date]['by_kind'][kind] += 1
            
            aggregated_timeline = list(timeline_by_date.values())
            aggregated_timeline.sort(key=lambda x: x['date'])
            
            # Age distribution
            age_distribution = {
                'new (0-1 days)': len([r for r in all_resources if r.age_days <= 1]),
                'recent (2-7 days)': len([r for r in all_resources if 1 < r.age_days <= 7]),
                'active (8-30 days)': len([r for r in all_resources if 7 < r.age_days <= 30]),
                'mature (31-90 days)': len([r for r in all_resources if 30 < r.age_days <= 90]),
                'stale (90+ days)': len([r for r in all_resources if r.age_days > 90])
            }
            
            # Most active namespaces (by resource creation)
            namespace_activity = {}
            for resource in all_resources:
                if resource.namespace:
                    namespace_activity[resource.namespace] = namespace_activity.get(resource.namespace, 0) + 1
            
            most_active_namespaces = [
                {'namespace': ns, 'resource_count': count}
                for ns, count in sorted(namespace_activity.items(), key=lambda x: x[1], reverse=True)[:10]
            ]
            
            # Creation patterns (by day of week)
            creation_patterns = {}
            for resource in all_resources:
                day_of_week = resource.created_at.strftime('%A')
                creation_patterns[day_of_week] = creation_patterns.get(day_of_week, 0) + 1
            
            # Resource lifecycle stats (average age by type)
            resource_lifecycle_stats = {}
            for resource in all_resources:
                kind = resource.resource_kind
                if kind not in resource_lifecycle_stats:
                    resource_lifecycle_stats[kind] = {'ages': [], 'avg_age': 0.0, 'count': 0}
                resource_lifecycle_stats[kind]['ages'].append(resource.age_days)
                resource_lifecycle_stats[kind]['count'] += 1
            
            # Calculate averages
            for kind, stats in resource_lifecycle_stats.items():
                if stats['ages']:
                    stats['avg_age'] = sum(stats['ages']) / len(stats['ages'])
                    stats['min_age'] = min(stats['ages'])
                    stats['max_age'] = max(stats['ages'])
                    del stats['ages']  # Remove raw data
            
            # Sort lists by relevance
            newest_resources.sort(key=lambda x: x.age_days)
            oldest_resources.sort(key=lambda x: x.age_days, reverse=True)
            stale_resources.sort(key=lambda x: x.age_days, reverse=True)
            
            return TemporalAnalysis(
                analysis_period=f"Last {days_back} days",
                total_resources=len(all_resources),
                creation_timeline=aggregated_timeline,
                update_timeline=[],  # Would need update tracking in schema
                age_distribution=age_distribution,
                most_active_namespaces=most_active_namespaces,
                newest_resources=newest_resources[:20],
                oldest_resources=oldest_resources[:20],
                stale_resources=stale_resources[:20],
                creation_patterns=creation_patterns,
                resource_lifecycle_stats=resource_lifecycle_stats
            )
    
    def get_namespace_storage_timeline(self, namespace: str) -> List[Dict[str, Any]]:
        """Get storage volume creation timeline for a namespace."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            cursor.execute("""
                SELECT creation_timestamp, name, kind, spec
                FROM resources
                WHERE namespace = ? AND kind IN ('PersistentVolumeClaim')
                AND creation_timestamp IS NOT NULL
                ORDER BY creation_timestamp
            """, (namespace,))
            
            timeline = []
            for row in cursor.fetchall():
                try:
                    created_at = datetime.fromisoformat(row['creation_timestamp'])
                    spec = json.loads(row['spec']) if row['spec'] else {}
                    
                    # Extract capacity
                    requests = spec.get('resources', {}).get('requests', {})
                    capacity = requests.get('storage', '')
                    capacity_gb = self._parse_storage_size(capacity) if capacity else 0.0
                    
                    timeline.append({
                        'date': created_at.strftime('%Y-%m-%d'),
                        'time': created_at.strftime('%H:%M'),
                        'name': row['name'],
                        'kind': row['kind'],
                        'capacity': capacity,
                        'capacity_gb': capacity_gb
                    })
                except (ValueError, json.JSONDecodeError):
                    continue
            
            return timeline
    
    def get_resource_efficiency(self) -> ResourceEfficiency:
        """Get resource efficiency analysis including pods without requests/limits."""
        with self.get_connection() as conn:
            cursor = conn.cursor()
            
            # Get all pods with their spec information
            cursor.execute("""
                SELECT name, namespace, spec, health_status, creation_timestamp
                FROM resources
                WHERE kind = 'Pod' AND spec IS NOT NULL
            """)
            
            pods_without_limits = 0
            pods_without_requests = 0
            pods_without_any_resources = 0
            pods_with_partial_resources = 0
            problematic_pods = []
            total_pods_analyzed = 0
            
            for row in cursor.fetchall():
                total_pods_analyzed += 1
                try:
                    spec = json.loads(row['spec'])
                    containers = spec.get('containers', [])
                    
                    pod_missing_requests = []
                    pod_missing_limits = []
                    container_names = []
                    
                    has_any_requests = False
                    has_any_limits = False
                    containers_with_issues = []
                    
                    for container in containers:
                        container_name = container.get('name', 'unknown')
                        container_names.append(container_name)
                        
                        resources = container.get('resources', {})
                        requests = resources.get('requests', {})
                        limits = resources.get('limits', {})
                        
                        # Check for missing requests
                        missing_cpu_request = 'cpu' not in requests
                        missing_memory_request = 'memory' not in requests
                        
                        # Check for missing limits
                        missing_cpu_limit = 'cpu' not in limits
                        missing_memory_limit = 'memory' not in limits
                        
                        if missing_cpu_request or missing_memory_request:
                            if missing_cpu_request and 'cpu' not in pod_missing_requests:
                                pod_missing_requests.append('cpu')
                            if missing_memory_request and 'memory' not in pod_missing_requests:
                                pod_missing_requests.append('memory')
                        else:
                            has_any_requests = True
                        
                        if missing_cpu_limit or missing_memory_limit:
                            if missing_cpu_limit and 'cpu' not in pod_missing_limits:
                                pod_missing_limits.append('cpu')
                            if missing_memory_limit and 'memory' not in pod_missing_limits:
                                pod_missing_limits.append('memory')
                        else:
                            has_any_limits = True
                        
                        # Track containers with issues
                        if missing_cpu_request or missing_memory_request or missing_cpu_limit or missing_memory_limit:
                            containers_with_issues.append(container_name)
                    
                    # Categorize pods
                    if pod_missing_requests:
                        pods_without_requests += 1
                    
                    if pod_missing_limits:
                        pods_without_limits += 1
                    
                    if not has_any_requests and not has_any_limits:
                        pods_without_any_resources += 1
                    
                    if (has_any_requests and pod_missing_requests) or (has_any_limits and pod_missing_limits):
                        pods_with_partial_resources += 1
                    
                    # Create issue entry if there are problems
                    if pod_missing_requests or pod_missing_limits:
                        # Determine severity
                        if not has_any_requests and not has_any_limits:
                            severity = 'critical'
                        elif len(pod_missing_requests) == 2 and len(pod_missing_limits) == 2:
                            severity = 'high'
                        elif len(pod_missing_requests) > 0 and len(pod_missing_limits) > 0:
                            severity = 'medium'
                        else:
                            severity = 'low'
                        
                        # Generate recommendations
                        recommendations = []
                        if 'cpu' in pod_missing_requests:
                            recommendations.append('Add CPU requests to ensure proper scheduling')
                        if 'memory' in pod_missing_requests:
                            recommendations.append('Add memory requests to prevent resource contention')
                        if 'cpu' in pod_missing_limits:
                            recommendations.append('Add CPU limits to prevent resource exhaustion')
                        if 'memory' in pod_missing_limits:
                            recommendations.append('Add memory limits to prevent OOM kills affecting other pods')
                        
                        if not has_any_requests and not has_any_limits:
                            recommendations.append('Pod has NO resource constraints - this can cause cluster instability')
                        
                        pod_issue = PodResourceIssue(
                            name=row['name'],
                            namespace=row['namespace'],
                            containers=container_names,
                            missing_requests=pod_missing_requests,
                            missing_limits=pod_missing_limits,
                            health_status=row['health_status'],
                            created_at=datetime.fromisoformat(row['creation_timestamp']) if row['creation_timestamp'] else None,
                            issue_severity=severity,
                            recommendations=recommendations
                        )
                        problematic_pods.append(pod_issue)
                        
                except (json.JSONDecodeError, KeyError, TypeError) as e:
                    # Skip pods with malformed spec
                    continue
            
            # Calculate resource coverage percentage
            pods_with_complete_resources = total_pods_analyzed - len(problematic_pods)
            resource_coverage_percentage = (pods_with_complete_resources / total_pods_analyzed * 100) if total_pods_analyzed > 0 else 0
            
            # Get additional efficiency metrics
            # Count unused ConfigMaps (simplified - ConfigMaps not referenced by any pod)
            cursor.execute("""
                SELECT COUNT(*) FROM resources 
                WHERE kind = 'ConfigMap' 
                AND uid NOT IN (
                    SELECT DISTINCT target_resource 
                    FROM relationships 
                    WHERE relationship_type = 'USES'
                )
            """)
            unused_config_maps = cursor.fetchone()[0]
            
            # Count orphaned PVCs
            cursor.execute("""
                SELECT COUNT(*) FROM resources 
                WHERE kind = 'PersistentVolumeClaim'
                AND json_extract(status, '$.phase') = 'Pending'
            """)
            orphaned_pvcs = cursor.fetchone()[0]
            
            # Count services without endpoints (simplified)
            cursor.execute("SELECT COUNT(*) FROM resources WHERE kind = 'Service'")
            services_without_endpoints = 0  # This would require more complex analysis
            
            # Sort problematic pods by severity
            severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}
            problematic_pods.sort(key=lambda x: (severity_order.get(x.issue_severity, 4), x.name))
            
            return ResourceEfficiency(
                pods_without_limits=pods_without_limits,
                pods_without_requests=pods_without_requests,
                pods_without_any_resources=pods_without_any_resources,
                pods_with_partial_resources=pods_with_partial_resources,
                over_requested_resources=[],  # Would need workload analysis
                unused_config_maps=unused_config_maps,
                orphaned_pvcs=orphaned_pvcs,
                services_without_endpoints=services_without_endpoints,
                problematic_pods=problematic_pods,
                resource_coverage_percentage=resource_coverage_percentage,
                total_pods_analyzed=total_pods_analyzed
            )
